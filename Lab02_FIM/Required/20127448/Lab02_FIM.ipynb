{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "DSYvP0n8RjYH"
   },
   "source": [
    "# Lab02: Frequent itemset mining\n",
    "\n",
    "- Student ID: 20127448\n",
    "- Student name: Nguyễn Thái Bảo\n",
    "\n",
    "**How to do your homework**\n",
    "\n",
    "\n",
    "You will work directly on this notebook; the word `TODO` indicate the parts you need to do.\n",
    "\n",
    "You can discuss ideas with classmates as well as finding information from the internet, book, etc...; but *this homework must be your*.\n",
    "\n",
    "**How to submit your homework**\n",
    "\n",
    "Before submitting, rerun the notebook (`Kernel` ->` Restart & Run All`).\n",
    "\n",
    "Then create a folder named `ID` (for example, if your ID is 1234567, then name the folder `1234567`) Copy file notebook to this folder, compress and submit it on moodle.\n",
    "\n",
    "**Contents:**\n",
    "\n",
    "- Frequent itemset mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXZ5gCVaRjYa"
   },
   "source": [
    "# 1. Preliminaries\n",
    "## This is how it all started ...\n",
    "- Rakesh Agrawal, Tomasz Imielinski, Arun N. Swami: Mining Association Rules between Sets of Items in Large Databases. SIGMOD Conference 1993: 207-216\n",
    "- Rakesh Agrawal, Ramakrishnan Srikant: Fast Algorithms for Mining Association Rules in Large Databases. VLDB 1994: 487-499\n",
    "\n",
    "**These two papers are credited with the birth of Data Mining**\n",
    "## Frequent itemset mining (FIM)\n",
    "\n",
    "Find combinations of items (itemsets) that occur frequently.\n",
    "## Applications\n",
    "- Items = products, transactions = sets of products someone bought in one trip to the store.\n",
    "$\\Rightarrow$ items people frequently buy together.\n",
    "    + Example: if people usually buy bread and coffee together, we run a sale of bread to attract people attention and raise price of coffee.\n",
    "- Items = webpages, transactions = words. Unusual words appearing together in a large number of documents, e.g., “Brad” and “Angelina,” may indicate an interesting relationship.\n",
    "- Transactions = Sentences, Items = Documents containing those sentences. Items that appear together too often could represent plagiarism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8vAJ8A2RjYi"
   },
   "source": [
    "## Transactional Database\n",
    "A transactional database $D$ consists of $N$ transactions: $D=\\left\\{T_1,T_2,...,T_N\\right\\}$. A transaction $T_n \\in D (1 \\le n \\le N)$ contains one or more items and that $I= \\left\\{ i_1,i_2,…,i_M \\right\\}$ is the set of distinct items in $D$, $T_n \\subset I$. Commonly, a transactional database is represented by a flat file instead of a database system: items are non-negative integers, each row represents a transaction, items in a transaction separated by space.\n",
    "\n",
    "Example: \n",
    "\n",
    "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 \n",
    "\n",
    "30 31 32 \n",
    "\n",
    "33 34 35 \n",
    "\n",
    "36 37 38 39 40 41 42 43 44 45 46 \n",
    "\n",
    "38 39 47 48 \n",
    "\n",
    "38 39 48 49 50 51 52 53 54 55 56 57 58 \n",
    "\n",
    "32 41 59 60 61 62 \n",
    "\n",
    "3 39 48 \n",
    "\n",
    "63 64 65 66 67 68 \n",
    "\n",
    "\n",
    "\n",
    "# Definition\n",
    "\n",
    "- Itemset: A collection of one or more items.\n",
    "    + Example: {1 4 5}\n",
    "- **k-itemset**: An itemset that contains k items.\n",
    "- Support: Frequency of occurrence of an itemset.\n",
    "    + Example: From the example above, item 3 appear in 2 transactions so its support is 2.\n",
    "- Frequent itemset: An itemset whose support is greater than or equal to a `minsup` threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdykKxr6RjY-"
   },
   "source": [
    "# The Apriori Principle\n",
    "- If an itemset is frequent, then all of its subsets must also be frequent.\n",
    "- If an itemset is not frequent, then all of its supersets cannot be frequent.\n",
    "- The support of an itemset never exceeds the support of its subsets.\n",
    "$$ \\forall{X,Y}: (X \\subseteq Y) \\Rightarrow s(X)\\ge s(Y)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvfMR7-CRjZB"
   },
   "source": [
    "# 2. Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9gZh4DORjZD"
   },
   "source": [
    "## The Apriori algorithm\n",
    "Suppose:\n",
    "\n",
    "$C_k$ candidate itemsets of size k.\n",
    "\n",
    "$L_k$ frequent itemsets of size k.\n",
    "\n",
    "The level-wise approach of Apriori algorithm can be descibed as follow:\n",
    "1. k=1, $C_k$ = all items.\n",
    "2. While $C_k$ not empty:\n",
    "    3. Scan the database to find which itemsets in $C_k$ are frequent and put them into $L_k$.\n",
    "    4. Use $L_k$ to generate a collection of candidate itemsets $C_{k+1}$ of size k+1.\n",
    "    5. k=k+1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qF9xHOBLRjZJ"
   },
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7F0lUOSuRjZN"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OogwdcLRjZf"
   },
   "source": [
    "### Read data\n",
    "First we have to read data from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "U2bsGrTERjZg"
   },
   "outputs": [],
   "source": [
    "def readData(path):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------\n",
    "        path: path of database D.\n",
    "         \n",
    "    --------------------------\n",
    "    Returns\n",
    "        data: a dictionary for representing database D\n",
    "                 - keys: transaction tids\n",
    "                 - values: itemsets.\n",
    "        s: support of distict items in D.\n",
    "    \"\"\"\n",
    "    data={}\n",
    "    s=defaultdict(lambda: 0) # Initialize a dictionary for storing support of items in I.  \n",
    "    with open(path,'rt') as f:\n",
    "        tid=1;\n",
    "        for line in f:\n",
    "            itemset=set(map(int,line.split())) # a python set is a native way for storing an itemset.\n",
    "            for item in itemset:  \n",
    "                s[item]+=1     #Why don't we compute support of items while reading data?\n",
    "            data[tid]= itemset\n",
    "            tid+=1\n",
    "    \n",
    "    return data, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSTC78WURjZu"
   },
   "source": [
    "### Tree Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGAkmuXtRjZw"
   },
   "source": [
    "**I gave you pseudo code of Apriori algorithm above but we implement Tree Projection. Tell me the differences of two algorithms.**\n",
    "\n",
    "\n",
    "**TODO:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BVRT5BnWRjZz"
   },
   "outputs": [],
   "source": [
    "def joinset(a, b):\n",
    "    '''\n",
    "    Parameters\n",
    "    -------------------\n",
    "        2 itemsets a and b (of course they are at same branch in search space)\n",
    "\n",
    "    -------------------\n",
    "    return\n",
    "        ret: itemset generated by joining a and b\n",
    "    '''\n",
    "    ret = list(set(a) | set(b))\n",
    "    return ret\n",
    "\n",
    "class TP:\n",
    "    def __init__(self, data=None, s=None, minSup=None):\n",
    "        self.data = data\n",
    "        self.s = {}\n",
    "\n",
    "        for key, support in sorted(s.items(), key=lambda item: item[1]):\n",
    "            self.s[key] = support\n",
    "        \n",
    "        self.minSup = minSup\n",
    "        self.L = {}  # Store frequent itemsets mined from database\n",
    "        self.runAlgorithm()\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"\n",
    "        Initialize search space at first step\n",
    "        --------------------------------------\n",
    "        We represent our search space in a tree structure\n",
    "        \"\"\"\n",
    "        tree = {}\n",
    "\n",
    "        search_space = {}\n",
    "        for item, support in self.s.items():\n",
    "            search_space[item] = {}\n",
    "\n",
    "            search_space[item]['itemset'] = [item]\n",
    "\n",
    "            search_space[item]['pruned'] = False\n",
    "\n",
    "            search_space[item]['support'] = support\n",
    "\n",
    "            tree[item] = {}\n",
    "        return tree, search_space\n",
    "    def computeItemsetSupport(self, itemset):\n",
    "        count = 0\n",
    "        for transaction in self.data:\n",
    "            if isinstance(transaction, set) and set(itemset).issubset(transaction):\n",
    "                count += 1\n",
    "        return count / len(self.data)\n",
    "\n",
    "\n",
    "    def get_sub_tree(self, k, tree, search_space, itter_node):\n",
    "        if k == 0:\n",
    "            return search_space[itter_node]['support']\n",
    "        subtree = search_space[itter_node]\n",
    "        for node in subtree.keys():\n",
    "            k-=1\n",
    "            self.get_sub_tree(k,tree,search_space,node)\n",
    "\n",
    "\n",
    "    def prune(self, k, tree, search_space):\n",
    "\n",
    "        '''\n",
    "        In this method we will find out which itemset in current search space is frequent\n",
    "        itemset then add it to L[k]. In addition, we prune those are not frequent itemsets.\n",
    "        '''\n",
    "        if self.L.get(k) is None: \n",
    "            self.L[k] = []\n",
    "        for item in search_space:\n",
    "            if search_space[item]['pruned'] is False and search_space[item]['support'] >= self.minSup:\n",
    "                self.L[k].append((search_space[item]['itemset'], search_space[item]['support']))\n",
    "                tree[item]['nodeLink'] = search_space[item]['support']\n",
    "            else:\n",
    "                search_space[item]['pruned'] = True\n",
    "\n",
    "\n",
    "\n",
    "    def generateSearchSpace(self, k, tree, search_space):\n",
    "        '''\n",
    "        Generate search space for exploring k+1 itemset. (Recursive function)\n",
    "        '''\n",
    "        items = list(tree.keys())\n",
    "        ''' print search_space.keys() you will understand  \n",
    "         why we need an additional tree, '''\n",
    "        l = len(items)\n",
    "        self.prune(k, tree, search_space)\n",
    "        if l == 0: return  # Stop condition\n",
    "        for i in range(l - 1):\n",
    "            sub_search_space = {}\n",
    "            sub_tree = {}\n",
    "            a = items[i]\n",
    "            if search_space[a]['pruned']: continue\n",
    "\n",
    "            for j in range(i + 1, l):\n",
    "                b = items[j]\n",
    "                search_space[a][b] = {}\n",
    "                tree[a][b] = {}\n",
    "                # You really need to understand what am i doing here before doing work below.\n",
    "                # (Hint: draw tree and search space to draft).\n",
    "\n",
    "                # First create newset using join set\n",
    "                newset = set(search_space[a]['itemset']).union(set(search_space[b]['itemset']))\n",
    "                \n",
    "                # Second add newset to search_space\n",
    "                sub_search_space[frozenset(newset)] = {}\n",
    "                sub_search_space[frozenset(newset)]['itemset'] = list(newset)\n",
    "                sub_search_space[frozenset(newset)]['pruned'] = False\n",
    "                sub_search_space[frozenset(newset)]['support'] = self.computeItemsetSupport(newset)\n",
    "                \n",
    "            #  Generate search_space for k+1-itemset\n",
    "            self.generateSearchSpace(k + 1, sub_tree, sub_search_space)\n",
    "\n",
    "    def runAlgorithm(self):\n",
    "        tree, search_space = self.initialize()  # generate search space for 1-itemset\n",
    "        self.generateSearchSpace(1, tree, search_space)\n",
    "\n",
    "    def miningResults(self):\n",
    "        return self.L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tMTpwxLRjZ-"
   },
   "source": [
    "Ok, let's test on a typical dataset `chess`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gLygYqiYRjZ-"
   },
   "outputs": [],
   "source": [
    "data, s= readData('chess.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PnxbU77YRjaF",
    "outputId": "c3b158be-6b46-4a3c-9b71-6a92d3d31ded"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [([48], 3013), ([56], 3021), ([66], 3021), ([34], 3040), ([62], 3060), ([7], 3076), ([36], 3099), ([60], 3149), ([40], 3170), ([29], 3181), ([52], 3185), ([58], 3195)], 2: []}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "a=TP(data=data,s=s, minSup=3000)\n",
    "print(a.miningResults())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Mp0RFbw-RjaU"
   },
   "source": [
    "### Answer questions here:\n",
    "**Why don't we compute support of items while reading data?**\\\n",
    "The reason we don't compute the support of items while reading data is that it would require us to traverse the entire database multiple times. Instead, we can compute the support of each item in a single pass through the database while reading the data. This approach is more efficient and allows us to build the data dictionary and the support dictionary simultaneously. Therefore, we can avoid extra computation time and make the process faster.\n",
    "\n",
    "**why should we do sort**\\\n",
    "The reason for sorting s and storing it in a new dictionary is to optimize the frequent itemset mining algorithm by making it more efficient in terms of time and memory usage.\n",
    "\n",
    "**study about python set and its advantages ?**\\\n",
    "One major advantage of sets is their ability to perform set operations such as union, intersection, and difference efficiently. Set operations in Python are performed using built-in methods, making it easy to work with sets and combine them with other data structures. Sets also provide a convenient way to remove duplicates from a list, which can be useful in data preprocessing.\n",
    "\n",
    "**After finish implementing the algorithm tell me why should you use this? Instead of delete item directly from search_space and tree.**\\\n",
    "The reason is that when an element is removed from search_space, all information related to that element is lost, including the elements associated with it. This will reduce flexibility and cause problems in managing and using search_space and tree later on. Instead, highlight the element by setting search_space[item]['pruned'] = True, so that later, if necessary, we can access that element in search_space and use information about It.\n",
    "\n",
    "**Apriori algorithm and Tree Projection, tell me the differences of two algorithms.**\\\n",
    "The Apriori algorithm exploits association rules by searching for all possible common subsets in a data set, and then using these subsets to generate larger sets.\\\n",
    "Tree Projection is a technique used in the Apriori algorithm to reduce computational complexity. It creates a tree structure called \"FP-Tree\" from the dataset, where each node represents an item in the dataset and its child nodes represent the items that appear associated with that item. FP-Tree helps to reduce the number of subsets to analyze by removing uncommon items from the data set and focusing only on common items.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnVm8wYIRjaV"
   },
   "source": [
    "# 3. Churn analysis\n",
    "\n",
    "In this section, you will use frequent itemset mining technique to analyze `churn` dataset (for any purposes). \n",
    "\n",
    "*Remember this dataset is not represented as a transactional database, first thing that you have to do is transforming it into a flat file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile():\n",
    "    f = open(\"churn.txt\", 'r')\n",
    "    i = 0\n",
    "    for line in f:\n",
    "        element = line.split(',')\n",
    "        if not element:\n",
    "            break\n",
    "        if i == 0:\n",
    "            t = len(element)\n",
    "            element[t-1] = element[t-1].strip('\\n')\n",
    "            col = element\n",
    "        else:\n",
    "            for i in range(len(element)):\n",
    "                element[i] = col[i]+\": \" + element[i].strip('. \\n')\n",
    "        yield element\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Churn_Analysis:\n",
    "    def __init__(self,minSup):\n",
    "        self.minSup = minSup \n",
    "        self.transaction = set() \n",
    "        self.L = defaultdict(int)  # Store frequent itemsets mined from database\n",
    "        self.data = readFile()\n",
    "\n",
    "    def frequent_size_1(self):\n",
    "        self.transaction = [set(row) for row in self.data]\n",
    "        frequent_1 = {frozenset([item]) for row in self.transaction for item in row}\n",
    "        return frequent_1\n",
    "    \n",
    "    def check_minSup(self, Ck):\n",
    "        prune = set() #store itemset satisfy minSup\n",
    "        cnt = defaultdict(int)\n",
    "        for tid in self.transaction:\n",
    "            for item in Ck:\n",
    "                if item.issubset(tid):\n",
    "                    self.L[item] += 1\n",
    "                    cnt[item] += 1\n",
    "        for key, support in cnt.items():\n",
    "            sup = support / len(self.transaction)\n",
    "            convertMinsup = round(self.minSup / len(self.transaction), 2)\n",
    "            if sup >= convertMinsup: \n",
    "                prune.add(key)\n",
    "        return prune\n",
    " \n",
    "    def make_new_item(self, itemset):\n",
    "        Ck = set()\n",
    "        itemset = sorted(itemset, key=lambda x: tuple(x))  # sắp xếp các phần tử trong các itemset\n",
    "        for i, a in enumerate(itemset):\n",
    "            for b in itemset[i + 1:]:  # tránh so sánh với các item đã được so sánh\n",
    "                c = a & b\n",
    "                if len(c) == len(a) - 1:\n",
    "                    Ck.add(a | b)\n",
    "        return Ck\n",
    "    \n",
    "    def apriori(self):\n",
    "        aprioriGen = dict()\n",
    "        frequent_1 = self.frequent_size_1()\n",
    "        Lk = self.check_minSup(frequent_1)\n",
    "        k = 2\n",
    "        if(len(Lk) == 0):\n",
    "            return\n",
    "        while(len(Lk) != 0):\n",
    "            aprioriGen[k - 1] = Lk\n",
    "            new_item = self.make_new_item(Lk)\n",
    "            scanDB = self.check_minSup(new_item)\n",
    "            Lk = scanDB\n",
    "            k += 1\n",
    "        result = []\n",
    "        for key, value in aprioriGen.items():\n",
    "            for item in value:\n",
    "                support = self.L.get(item) / len(self.transaction)\n",
    "                if support >= (self.minSup / 3334):\n",
    "                    result.extend([(set(item), support)])  \n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'VMail Message: 0'} with support: 0.72\n",
      "{'VMail Plan: no'} with support: 0.72\n",
      "{\"Int'l Plan: no\"} with support: 0.9\n",
      "{'Area Code: 415'} with support: 0.5\n",
      "{'Churn?: False'} with support: 0.85\n",
      "{\"Int'l Plan: no\", 'Churn?: False'} with support: 0.8\n",
      "{\"Int'l Plan: no\", 'VMail Message: 0'} with support: 0.65\n",
      "{'Churn?: False', 'Area Code: 415'} with support: 0.43\n",
      "{'VMail Plan: no', 'Churn?: False'} with support: 0.6\n",
      "{\"Int'l Plan: no\", 'Area Code: 415'} with support: 0.45\n",
      "{'VMail Plan: no', 'VMail Message: 0'} with support: 0.72\n",
      "{'Churn?: False', 'VMail Message: 0'} with support: 0.6\n",
      "{'VMail Plan: no', \"Int'l Plan: no\"} with support: 0.65\n",
      "{'VMail Plan: no', 'Churn?: False', 'VMail Message: 0'} with support: 0.6\n",
      "{'VMail Plan: no', \"Int'l Plan: no\", 'Churn?: False'} with support: 0.56\n",
      "{\"Int'l Plan: no\", 'Churn?: False', 'VMail Message: 0'} with support: 0.56\n",
      "{'VMail Plan: no', \"Int'l Plan: no\", 'VMail Message: 0'} with support: 0.65\n",
      "{'VMail Plan: no', \"Int'l Plan: no\", 'Churn?: False', 'VMail Message: 0'} with support: 0.56\n",
      "We have 18 frequent itemset with minsup = 0.4 ~ 1333\n"
     ]
    }
   ],
   "source": [
    "# 1333 is 0.4 minSup\n",
    "a = Churn_Analysis(1333)\n",
    "count = 0\n",
    "for key, support in a.apriori():\n",
    "    count +=1\n",
    "    print(f\"{set(key)} with support: {round(support, 2)}\")\n",
    "print(\"We have {0} frequent itemset with minsup = 0.4 ~ 1333\".format(count))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The firstly , i collect dataset from https://github.com/srepho/srepho.github.io/blob/master/Churn/churn.txt and create new file with name `churn.txt`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class Churn_Analysis includes the following methods:\n",
    "\n",
    "1. Initialization method: takes a minSup parameter representing support threshold and initializes transaction instance variables (stores transactions), L (stores support count of itemsets), and data (reads data from file). ).\n",
    "2. The frequent_size_1 method: generates all itemsets of size 1 from transactions and returns the set frequent_1.\n",
    "3. The check_minSup method: takes a set of Ck and counts the number of occurrences of itemsets in Ck in transactions. If the itemset has support greater than or equal to minSup, it is added to the prune set and prune is returned.\n",
    "4. make_new_item method: creates new itemsets by combining itemsets of size k-1 to form itemsets of size k.\n",
    "5. apriori method: use the above methods to implement the Apriori algorithm and return the frequent itemsets with support greater than or equal to minSup.\n",
    ">In the apriori method, the frequent itemset set is printed and the number of frequent itemsets found is counted. Support threshold is calculated by dividing minSup by the number of transactions in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FzxGs7RRjaX"
   },
   "source": [
    "# 4 References"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "doYH4biqR_N7"
   },
   "source": [
    "Feel free to send questions to my email address: ntthuhang0131@gmail.com\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Lab01 - Frequent itemset mining.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
